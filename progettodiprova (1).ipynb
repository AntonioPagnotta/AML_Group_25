{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Percorso al file dentro Google Drive\n",
    "#file_path = '/content/drive/MyDrive/AML_project/embeddings.pkl'\n",
    "\n",
    "file_path = 'embeddings.pkl'\n",
    "\n",
    "# Leggere un file pickle\n",
    "import pickle\n",
    "with open(file_path, 'rb') as f:\n",
    "    data = pickle.load(f)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E4Dz68CIzzDA",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765215267312,
     "user_tz": -60,
     "elapsed": 13784,
     "user": {
      "displayName": "Andrea SCALISE",
      "userId": "06123552028920003905"
     }
    },
    "outputId": "9384b4c5-bc94-4231-9b0d-3b63f20c8aec",
    "ExecuteTime": {
     "end_time": "2025-12-09T18:15:49.781001Z",
     "start_time": "2025-12-09T18:15:45.784954Z"
    }
   },
   "id": "E4Dz68CIzzDA",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/nm4pm2012rj1j5k7yc6x23bw0000gn/T/ipykernel_38767/1449163981.py:12: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "%pip install torch"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NAtDVyNU_zGR",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765215223870,
     "user_tz": -60,
     "elapsed": 4542,
     "user": {
      "displayName": "Andrea SCALISE",
      "userId": "06123552028920003905"
     }
    },
    "outputId": "c911eaf6-7df2-493a-c16f-11d41e61151b",
    "ExecuteTime": {
     "end_time": "2025-12-09T18:15:52.718004Z",
     "start_time": "2025-12-09T18:15:49.660666Z"
    }
   },
   "id": "NAtDVyNU_zGR",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (2.9.0)\r\n",
      "Requirement already satisfied: filelock in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.16.1)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (4.15.0)\r\n",
      "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (74.0.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (1.13.3)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.3)\r\n",
      "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (3.1.3)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from torch) (2024.9.0)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from sympy>=1.13.3->torch) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\r\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import f1_score, mean_squared_error, r2_score\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "class Config:\n",
    "    # UPDATE THIS PATH TO YOUR FILE\n",
    "    data_path = 'embeddings.pkl' \n",
    "    \n",
    "    epochs = 10\n",
    "    batch_size = 256\n",
    "    \n",
    "    # Hyperbolic Params\n",
    "    hyp_lr = 1e-4\n",
    "    hyp_temp = 0.1\n",
    "    hyp_dim = 128\n",
    "    \n",
    "    # Euclidean Params\n",
    "    euc_lr = 1e-3\n",
    "    euc_hidden = [1024, 512]\n",
    "    \n",
    "    val_frac = 0.2\n",
    "    seed = 42\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# --- UTILS & DATASET ---\n",
    "\n",
    "def discretize_fitness(fitness_array):\n",
    "    \"\"\"Transforms continuous fitness into 5 discrete classes.\"\"\"\n",
    "    bins = [-np.inf, -3.0, -1.0, 1.0, 3.0, np.inf]\n",
    "    labels = np.digitize(fitness_array, bins) - 1\n",
    "    return labels\n",
    "\n",
    "class MutationDataset(Dataset):\n",
    "    def __init__(self, pkl_path):\n",
    "        with open(pkl_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "\n",
    "        # 1. Handle different pkl structures (DataFrame vs Dict)\n",
    "        if hasattr(data, 'loc'): \n",
    "             self.fitness = data['fitness'].values\n",
    "             embeddings_raw = data['embedding'].values\n",
    "             self.file_names = data['file_name'].values\n",
    "        else: \n",
    "             self.fitness = np.array(data['fitness'])\n",
    "             embeddings_raw = np.array(data['embedding'])\n",
    "             self.file_names = np.array(data['file_name'])\n",
    "\n",
    "        self.labels = discretize_fitness(self.fitness)\n",
    "        \n",
    "        # 2. FIX: Padding Logic to handle variable-length embeddings\n",
    "        # First, convert everything to numpy arrays and check shapes\n",
    "        np_embs = [np.array(e) for e in embeddings_raw]\n",
    "        shapes = [e.shape for e in np_embs]\n",
    "        \n",
    "        # Calculate max height and width\n",
    "        max_h = max(s[0] for s in shapes)\n",
    "        # Handle case where embedding might be 1D\n",
    "        max_w = max(s[1] for s in shapes) if len(shapes[0]) > 1 else 1\n",
    "        \n",
    "        print(f\"Dataset Stats: Max Height={max_h}, Max Width={max_w}\")\n",
    "        \n",
    "        processed_embs = []\n",
    "        for arr in np_embs:\n",
    "            # Create a zero-filled array of the max shape\n",
    "            if len(arr.shape) == 1:\n",
    "                padded = np.zeros((max_h,), dtype=np.float32)\n",
    "                padded[:arr.shape[0]] = arr\n",
    "            else:\n",
    "                padded = np.zeros((max_h, max_w), dtype=np.float32)\n",
    "                padded[:arr.shape[0], :arr.shape[1]] = arr\n",
    "            \n",
    "            # Flatten to 1D vector for MLP input\n",
    "            processed_embs.append(padded.flatten())\n",
    "        \n",
    "        # Now stacking works because all arrays are the same size\n",
    "        self.embeddings = np.array(processed_embs, dtype=np.float32)\n",
    "        \n",
    "        # 3. Normalize input data (Z-score normalization)\n",
    "        # Essential for Hyperbolic stability\n",
    "        mean = self.embeddings.mean(axis=0)\n",
    "        std = self.embeddings.std(axis=0) + 1e-6\n",
    "        self.embeddings = (self.embeddings - mean) / std\n",
    "\n",
    "        self.input_dim = self.embeddings.shape[1]\n",
    "        \n",
    "        # Parse proteins for InfoNCE\n",
    "        self.proteins = np.array([str(fn).split('_')[0] for fn in self.file_names])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.embeddings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'embedding': torch.tensor(self.embeddings[idx]),\n",
    "            'fitness': torch.tensor(self.fitness[idx], dtype=torch.float32), \n",
    "            'label': torch.tensor(self.labels[idx], dtype=torch.long),\n",
    "            'protein': self.proteins[idx]\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    embeddings = torch.stack([b['embedding'] for b in batch])\n",
    "    fitness = torch.stack([b['fitness'] for b in batch])\n",
    "    labels = torch.stack([b['label'] for b in batch])\n",
    "    proteins = [b['protein'] for b in batch]\n",
    "    return embeddings, fitness, labels, proteins\n",
    "\n",
    "# ==========================================\n",
    "# 1. FIXED HYPERBOLIC MODEL (Classification)\n",
    "# ==========================================\n",
    "\n",
    "# Using float64 to prevent NaN\n",
    "torch.set_default_dtype(torch.float64) \n",
    "\n",
    "class HyperbolicUtils:\n",
    "    EPS = 1e-5\n",
    "    \n",
    "    @staticmethod\n",
    "    def artanh(x):\n",
    "        x = torch.clamp(x, min=-1.0 + 1e-5, max=1.0 - 1e-5)\n",
    "        return 0.5 * torch.log((1 + x) / (1 - x))\n",
    "\n",
    "    @staticmethod\n",
    "    def exp_map_zero(v):\n",
    "        v_norm = torch.norm(v, p=2, dim=-1, keepdim=True).clamp_min(1e-6)\n",
    "        scaled = torch.tanh(v_norm) * (v / v_norm)\n",
    "        return scaled\n",
    "\n",
    "class HyperbolicClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, hyp_dim, num_classes=5):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_d = input_dim\n",
    "        \n",
    "        # Euclidean Encoder Layers\n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_d, h_dim))\n",
    "            layers.append(nn.LayerNorm(h_dim)) # Added Norm\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "            in_d = h_dim\n",
    "            \n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        self.to_hyp = nn.Linear(in_d, hyp_dim)\n",
    "        self.classifier = nn.Linear(hyp_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.double()\n",
    "        feat = self.encoder(x)\n",
    "        tangent = self.to_hyp(feat)\n",
    "        \n",
    "        # Tangent Clipping (Crucial for stability)\n",
    "        norm = torch.norm(tangent, p=2, dim=-1, keepdim=True)\n",
    "        scale = torch.clamp(norm, max=5.0) / (norm + 1e-6)\n",
    "        tangent = tangent * scale\n",
    "        \n",
    "        # Map to Hyperbolic space (for potential metric learning)\n",
    "        hyp_emb = HyperbolicUtils.exp_map_zero(tangent)\n",
    "        \n",
    "        # Logits from Tangent space (standard MLR in tangent space)\n",
    "        logits = self.classifier(tangent)\n",
    "        \n",
    "        return hyp_emb, logits\n",
    "\n",
    "# ==========================================\n",
    "# 2. EUCLIDEAN MODEL (Regression)\n",
    "# ==========================================\n",
    "\n",
    "class EuclideanRegressor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims=[1024, 512]):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        in_d = input_dim\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(in_d, h_dim))\n",
    "            layers.append(nn.BatchNorm1d(h_dim)) \n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.1))\n",
    "            in_d = h_dim\n",
    "            \n",
    "        # Output dimension 1 for Regression\n",
    "        layers.append(nn.Linear(in_d, 1))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float() # Use float32 for speed\n",
    "        return self.model(x).squeeze()\n",
    "\n",
    "# ==========================================\n",
    "# 3. TRAINING LOOPS\n",
    "# ==========================================\n",
    "\n",
    "def train_hyperbolic(config, train_loader, val_loader, input_dim):\n",
    "    print(\"\\n--- Training Hyperbolic Model (Classification) ---\")\n",
    "    model = HyperbolicClassifier(input_dim, [512], config.hyp_dim).to(config.device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=config.hyp_lr)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for embs, _, labels, _ in train_loader:\n",
    "            embs, labels = embs.to(config.device), labels.to(config.device)\n",
    "            \n",
    "            _, logits = model(embs)\n",
    "            loss = F.cross_entropy(logits, labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        preds, true_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for embs, _, labels, _ in val_loader:\n",
    "                embs = embs.to(config.device)\n",
    "                _, logits = model(embs)\n",
    "                preds.append(logits.argmax(dim=-1).cpu().numpy())\n",
    "                true_labels.append(labels.numpy())\n",
    "        \n",
    "        y_pred = np.concatenate(preds)\n",
    "        y_true = np.concatenate(true_labels)\n",
    "        f1 = f1_score(y_true, y_pred, average='weighted', zero_division=0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss/len(train_loader):.4f} | Val F1: {f1:.4f}\")\n",
    "        if f1 > best_f1: best_f1 = f1\n",
    "        \n",
    "    return best_f1\n",
    "\n",
    "def train_euclidean(config, train_loader, val_loader, input_dim):\n",
    "    print(\"\\n--- Training Euclidean Model (Regression) ---\")\n",
    "    # Switch context to float32 for Euclidean model\n",
    "    torch.set_default_dtype(torch.float32)\n",
    "    \n",
    "    model = EuclideanRegressor(input_dim, config.euc_hidden).to(config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.euc_lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    best_reg_f1 = 0.0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for embs, fitness, _, _ in train_loader:\n",
    "            embs = embs.to(config.device).float()\n",
    "            fitness = fitness.to(config.device).float()\n",
    "            \n",
    "            pred = model(embs)\n",
    "            loss = criterion(pred, fitness)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_preds, all_fitness = [], []\n",
    "        with torch.no_grad():\n",
    "            for embs, fitness, _, _ in val_loader:\n",
    "                embs = embs.to(config.device).float()\n",
    "                pred = model(embs)\n",
    "                all_preds.append(pred.cpu().numpy())\n",
    "                all_fitness.append(fitness.numpy())\n",
    "        \n",
    "        y_pred = np.concatenate(all_preds)\n",
    "        y_true = np.concatenate(all_fitness)\n",
    "        \n",
    "        # Convert Regression output to Classification bins for fair comparison\n",
    "        y_pred_cls = discretize_fitness(y_pred)\n",
    "        y_true_cls = discretize_fitness(y_true)\n",
    "        f1_as_cls = f1_score(y_true_cls, y_pred_cls, average='weighted', zero_division=0)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} | MSE: {mean_squared_error(y_true, y_pred):.4f} | Equivalent F1: {f1_as_cls:.4f}\")\n",
    "        \n",
    "        if f1_as_cls > best_reg_f1: \n",
    "            best_reg_f1 = f1_as_cls\n",
    "            \n",
    "    return best_reg_f1\n",
    "\n",
    "# --- MAIN ---\n",
    "\n",
    "def main():\n",
    "    if not os.path.exists(Config.data_path):\n",
    "        print(f\"File not found: {Config.data_path}. Please check the path.\")\n",
    "        return\n",
    "\n",
    "    # Data Loading\n",
    "    print(\"Loading Dataset...\")\n",
    "    dataset = MutationDataset(Config.data_path)\n",
    "    \n",
    "    train_size = int((1 - Config.val_frac) * len(dataset))\n",
    "    val_size = len(dataset) - train_size\n",
    "    train_ds, val_ds = torch.utils.data.random_split(\n",
    "        dataset, [train_size, val_size], \n",
    "        generator=torch.Generator().manual_seed(Config.seed)\n",
    "    )\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=Config.batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "    val_loader = DataLoader(val_ds, batch_size=Config.batch_size, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "    print(f\"Dataset Loaded. Input Dim (Flattened): {dataset.input_dim}\")\n",
    "    \n",
    "    # 1. Train Hyperbolic (Classification)\n",
    "    hyp_f1 = train_hyperbolic(Config, train_loader, val_loader, dataset.input_dim)\n",
    "    \n",
    "    # 2. Train Euclidean (Regression)\n",
    "    euc_f1 = train_euclidean(Config, train_loader, val_loader, dataset.input_dim)\n",
    "    \n",
    "    print(\"\\n================ FINAL COMPARISON ================\")\n",
    "    print(f\"Hyperbolic (Classification) Weighted F1: {hyp_f1:.4f}\")\n",
    "    print(f\"Euclidean (Regression -> Class) Weighted F1: {euc_f1:.4f}\")\n",
    "    \n",
    "    if euc_f1 > hyp_f1:\n",
    "        print(\">> Euclidean Regression performed better.\")\n",
    "    else:\n",
    "        print(\">> Hyperbolic Classification performed better.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main() "
   ],
   "metadata": {
    "id": "8dvtU_Pk7HzE",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765215238496,
     "user_tz": -60,
     "elapsed": 8079,
     "user": {
      "displayName": "Andrea SCALISE",
      "userId": "06123552028920003905"
     }
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2025-12-09T18:15:55.235763Z"
    }
   },
   "id": "8dvtU_Pk7HzE",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vw/nm4pm2012rj1j5k7yc6x23bw0000gn/T/ipykernel_38767/800778052.py:43: DeprecationWarning: numpy.core.numeric is deprecated and has been renamed to numpy._core.numeric. The numpy._core namespace contains private NumPy internals and its use is discouraged, as NumPy internals can change without warning in any release. In practice, most real-world usage of numpy.core is to access functionality in the public NumPy API. If that is the case, use the public NumPy API. If not, you are using NumPy internals. If you would still like to access an internal attribute, use numpy._core.numeric._frombuffer.\n",
      "  data = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats: Max Height=768, Max Width=384\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "torch.device(\"cuda\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7l9BIqPF_bjc",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765215246537,
     "user_tz": -60,
     "elapsed": 18,
     "user": {
      "displayName": "Andrea SCALISE",
      "userId": "06123552028920003905"
     }
    },
    "outputId": "fb909cc8-c2b5-4b5a-c953-36a24ff7d040"
   },
   "id": "7l9BIqPF_bjc",
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Applica la discretizzazione\n",
    "labels = discretize_fitness(data[\"fitness\"])\n",
    "\n",
    "# Conta il numero di valori per ciascuna classe (0..4)\n",
    "unique, counts = np.unique(labels, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"Classe {cls}: {count} valori\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YVCi5P8w4Hih",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765212875884,
     "user_tz": -60,
     "elapsed": 11,
     "user": {
      "displayName": "Andrea SCALISE",
      "userId": "06123552028920003905"
     }
    },
    "outputId": "40d15dcd-048c-44c8-a01b-0bbd43a6a878",
    "ExecuteTime": {
     "end_time": "2025-12-09T17:22:49.410070Z",
     "start_time": "2025-12-09T17:22:49.367380Z"
    }
   },
   "id": "YVCi5P8w4Hih",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe 0: 824 valori\n",
      "Classe 1: 977 valori\n",
      "Classe 2: 1800 valori\n",
      "Classe 3: 208 valori\n",
      "Classe 4: 107 valori\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bb3e6301647d844f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
